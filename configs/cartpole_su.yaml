# Environment configuration
env: CartpoleSwingUp              # env name
train_params:                     # model contexts for training
- polelength: [0.4, 0.6, 0.8, 1., 1.2]
  masspole: [0.75, 1.0, 1.25]
  # masscart: [0.25, 0.5, 1.5, 2.5]
  # gravity: [8.8, 12.0]
  # force_mag: [18.0, 22.0]
test_range:                       # model contexts for testing
- polelength: [0.2, 0.5, 0.9, 1.4]
  masspole: [0.7, 0.9, 1.4]
  # masscart: [1.5, 2.5]
  # gravity: [9.8, 12.8]
  # force_mag: [10.0, 15.0]
normalize_flag: False              # normalize input

trail_length: 200                 # max len of episode
init_trials: 5                   # Number of initial trials to fill replay buffer
num_trials: 5                    # Number of episodes w/ agent actions
replay_buffer_sz: 10000

reward_fn: False                  # Use reward function or not

# Agent configuration
agent:
  horizon: 50                     # how long of the horizon to predict
  popsize: 200                    # how many random samples for mpc
  alpha: 0.1
  replan_freq: 1
  num_iters: 5
  elite_ratio: 0.1
  max_particles: 20
  eval:                           # Properties of action sequence evaluation fn
    method: kl                # Method to use 'default', 'greedy', 'kl', 'combine'
    mc_samples: 50                # Samples from context to consider
    pred_weight: 1.               # Weight for pred_reward
    kl_weight: 1.                 # Weight for kl_distance

# Model arch parameters
context:
  history_size: 50
  hidden_dim: 200
  hidden_layers: 1
  out_dim: 16
  hidden_actv: relu
  output_actv: identity
  no_context: False

stateaction:
  hidden_dim: 200
  hidden_layers: 1
  out_dim: 256
  hidden_actv: relu
  state_actv: identity
  action_actv: tanh

transitionreward:
  hidden_dim: 200
  hidden_layers: 3
  ensemble_size: 5
  prop_method: fixed_model
  actv: torch.nn.LeakyReLU

# Dynamics model training config
dynamics:
  learning_rate: 0.001
  batch_size: 256
  validation_ratio: 0.05         # ratio of validation set
  epochs_per_step: 50
  patience: 20

# "Trajectory-wise Multiple Choice Learning for Dynamics Generalization
# in Reinforcement Learning" parameters
TW:
  history_length: 10
  future_length: 10
  state_diff: 0
  non_adaptive_planning: 0
